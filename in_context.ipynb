{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to use ollam and model llama3.1:latest and in-context learning to do credit card fraud detection. And test the performance of the model.\n",
    "\n",
    "# the huggingface dataset for legitimate transactions is called \"yunfan-y/fraud-detection-legitimate\"\n",
    "\n",
    "# the huggingface dataset for fraudulent transactions is called \"yunfan-y/fraud-detection-fraud\"\n",
    "\n",
    "# those datasets have train, validation, and test splits. You should use the test split to test the performance of the model.\n",
    "\n",
    "# all datasets have columns \"conversation\" and \"response\" \n",
    "# the response is either \"LEGITIMATE\" or \"FRAUD\"\n",
    "\n",
    "# here is a sample data: \n",
    "\n",
    "# conversation: Transaction Details: - Date/Time: 2019-05-26 05:20:36 - Merchant: fraud_Romaguera, Cruickshank and Greenholt - Amount: $104.9 - Category: shopping_net - Gender: M - State: OR\n",
    "\n",
    "# response: LEGITIMATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aiden\\AppData\\Local\\anaconda3\\envs\\hugging_face\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Aiden\\AppData\\Local\\anaconda3\\envs\\hugging_face\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aiden\\.cache\\huggingface\\hub\\datasets--yunfan-y--fraud-detection-legitimate. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 80000/80000 [00:00<00:00, 2191381.40 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 2917370.80 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 2248594.86 examples/s]\n",
      "c:\\Users\\Aiden\\AppData\\Local\\anaconda3\\envs\\hugging_face\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aiden\\.cache\\huggingface\\hub\\datasets--yunfan-y--fraud-detection-fraud. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 6004/6004 [00:00<00:00, 2991873.73 examples/s]\n",
      "Generating validation split: 100%|██████████| 751/751 [00:00<00:00, 751412.76 examples/s]\n",
      "Generating test split: 100%|██████████| 751/751 [00:00<00:00, 749981.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load legitimate transactions (test split)\n",
    "legitimate_test = load_dataset(\"yunfan-y/fraud-detection-legitimate\", split=\"test\")\n",
    "\n",
    "# Load fraudulent transactions (test split)\n",
    "fraud_test = load_dataset(\"yunfan-y/fraud-detection-fraud\", split=\"test\")\n",
    "\n",
    "# Combine the datasets\n",
    "test_dataset = concatenate_datasets([legitimate_test, fraud_test])\n",
    "\n",
    "# Shuffle the dataset\n",
    "test_dataset = test_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/500 examples\n",
      "Processed 20/500 examples\n",
      "Processed 30/500 examples\n",
      "Processed 40/500 examples\n",
      "Processed 50/500 examples\n",
      "Processed 60/500 examples\n",
      "Processed 70/500 examples\n",
      "Processed 80/500 examples\n",
      "Processed 90/500 examples\n",
      "Processed 100/500 examples\n",
      "Processed 110/500 examples\n",
      "Processed 120/500 examples\n",
      "Processed 130/500 examples\n",
      "Processed 140/500 examples\n",
      "Processed 150/500 examples\n",
      "Processed 160/500 examples\n",
      "Processed 170/500 examples\n",
      "Processed 180/500 examples\n",
      "Processed 190/500 examples\n",
      "Processed 200/500 examples\n",
      "Processed 210/500 examples\n",
      "Processed 220/500 examples\n",
      "Processed 230/500 examples\n",
      "Processed 240/500 examples\n",
      "Processed 250/500 examples\n",
      "Processed 260/500 examples\n",
      "Processed 270/500 examples\n",
      "Processed 280/500 examples\n",
      "Processed 290/500 examples\n",
      "Processed 300/500 examples\n",
      "Processed 310/500 examples\n",
      "Processed 320/500 examples\n",
      "Processed 330/500 examples\n",
      "Processed 340/500 examples\n",
      "Processed 350/500 examples\n",
      "Processed 360/500 examples\n",
      "Processed 370/500 examples\n",
      "Processed 380/500 examples\n",
      "Processed 390/500 examples\n",
      "Processed 400/500 examples\n",
      "Processed 410/500 examples\n",
      "Processed 420/500 examples\n",
      "Processed 430/500 examples\n",
      "Processed 440/500 examples\n",
      "Processed 450/500 examples\n",
      "Processed 460/500 examples\n",
      "Processed 470/500 examples\n",
      "Processed 480/500 examples\n",
      "Processed 490/500 examples\n",
      "Processed 500/500 examples\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FRAUD       0.40      0.59      0.48        41\n",
      "  LEGITIMATE       0.96      0.92      0.94       459\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.68      0.75      0.71       500\n",
      "weighted avg       0.92      0.89      0.90       500\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 24  17]\n",
      " [ 36 423]]\n"
     ]
    }
   ],
   "source": [
    "# ... existing dataset loading code ...\n",
    "\n",
    "from ollama import Client\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "# Function to create prompt with in-context examples\n",
    "def create_prompt(test_example, num_examples=5):\n",
    "    # Randomly sample examples from training set (excluding the test example)\n",
    "    train_examples = test_dataset.select(\n",
    "        random.sample(range(len(test_dataset)), num_examples)\n",
    "    )\n",
    "    \n",
    "    # Build prompt with examples\n",
    "    prompt = \"You are a fraud detection system. Analyze these transactions and determine if they are LEGITIMATE or FRAUD. Reply with either LEGITIMATE or FRAUD only. Use the following examples to help you make the decision.\\n\\n\"\n",
    "    \n",
    "    # Add training examples\n",
    "    for example in train_examples:\n",
    "        prompt += f\"Transaction: {example['conversation']}\\nResult: {example['response']}\\n\\n\"\n",
    "    \n",
    "    # Add test example\n",
    "    prompt += f\"Transaction: {test_example['conversation']}\\nResult:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(num_samples=100):\n",
    "    results = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Sample subset for testing\n",
    "    test_subset = test_dataset.select(range(num_samples))\n",
    "    \n",
    "    for idx, example in enumerate(test_subset):\n",
    "        try:\n",
    "            # Create prompt with in-context examples\n",
    "            prompt = create_prompt(example)\n",
    "            \n",
    "            # Get model prediction\n",
    "            response = client.generate(\n",
    "                model='llama3.1:latest',\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'num_predict': 10,  # replaced max_tokens with num_predict\n",
    "                    'temperature': 0.1  # adding low temperature for more consistent results\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Extract prediction (assuming model outputs LEGITIMATE or FRAUD)\n",
    "            prediction = 'LEGITIMATE' if 'LEGITIMATE' in response['response'] else 'FRAUD'\n",
    "            \n",
    "            results.append(prediction)\n",
    "            true_labels.append(example['response'])\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{num_samples} examples\")\n",
    "                \n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, results))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(true_labels, results))\n",
    "    \n",
    "    return results, true_labels\n",
    "\n",
    "# Run evaluation\n",
    "results, true_labels = evaluate_model(num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/500 examples\n",
      "Processed 20/500 examples\n",
      "Processed 30/500 examples\n",
      "Processed 40/500 examples\n",
      "Processed 50/500 examples\n",
      "Processed 60/500 examples\n",
      "Processed 70/500 examples\n",
      "Processed 80/500 examples\n",
      "Processed 90/500 examples\n",
      "Processed 100/500 examples\n",
      "Processed 110/500 examples\n",
      "Processed 120/500 examples\n",
      "Processed 130/500 examples\n",
      "Processed 140/500 examples\n",
      "Processed 150/500 examples\n",
      "Processed 160/500 examples\n",
      "Processed 170/500 examples\n",
      "Processed 180/500 examples\n",
      "Processed 190/500 examples\n",
      "Processed 200/500 examples\n",
      "Processed 210/500 examples\n",
      "Processed 220/500 examples\n",
      "Processed 230/500 examples\n",
      "Processed 240/500 examples\n",
      "Processed 250/500 examples\n",
      "Processed 260/500 examples\n",
      "Processed 270/500 examples\n",
      "Processed 280/500 examples\n",
      "Processed 290/500 examples\n",
      "Processed 300/500 examples\n",
      "Processed 310/500 examples\n",
      "Processed 320/500 examples\n",
      "Processed 330/500 examples\n",
      "Processed 340/500 examples\n",
      "Processed 350/500 examples\n",
      "Processed 360/500 examples\n",
      "Processed 370/500 examples\n",
      "Processed 380/500 examples\n",
      "Processed 390/500 examples\n",
      "Processed 400/500 examples\n",
      "Processed 410/500 examples\n",
      "Processed 420/500 examples\n",
      "Processed 430/500 examples\n",
      "Processed 440/500 examples\n",
      "Processed 450/500 examples\n",
      "Processed 460/500 examples\n",
      "Processed 470/500 examples\n",
      "Processed 480/500 examples\n",
      "Processed 490/500 examples\n",
      "Processed 500/500 examples\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FRAUD       0.12      0.93      0.21        41\n",
      "  LEGITIMATE       0.98      0.37      0.54       459\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.55      0.65      0.38       500\n",
      "weighted avg       0.91      0.42      0.52       500\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 38   3]\n",
      " [287 172]]\n"
     ]
    }
   ],
   "source": [
    "# ... existing dataset loading code ...\n",
    "\n",
    "from ollama import Client\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "# Function to create prompt with in-context examples\n",
    "def create_prompt(test_example, num_examples=5):\n",
    "    # Randomly sample examples from training set (excluding the test example)\n",
    "    train_examples = test_dataset.select(\n",
    "        random.sample(range(len(test_dataset)), num_examples)\n",
    "    )\n",
    "    \n",
    "    # Build prompt with examples\n",
    "    prompt = \"Your job is to detect credit card fraud. I will give you 5 examples first and another data point. You will tell me if this data point is LEGITIMATE or FRAUD. Reply with either LEGITIMATE or FRAUD only.\\n\\n\"\n",
    "    \n",
    "    # Add training examples\n",
    "    for example in train_examples:\n",
    "        prompt += f\"Transaction: {example['conversation']}\\nResult: {example['response']}\\n\\n\"\n",
    "    \n",
    "    # Add test example\n",
    "    prompt += f\"Transaction: {test_example['conversation']}\\nResult:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(num_samples=100):\n",
    "    results = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Sample subset for testing\n",
    "    test_subset = test_dataset.select(range(num_samples))\n",
    "    \n",
    "    for idx, example in enumerate(test_subset):\n",
    "        try:\n",
    "            # Create prompt with in-context examples\n",
    "            prompt = create_prompt(example)\n",
    "            \n",
    "            # Get model prediction\n",
    "            response = client.generate(\n",
    "                model='gemma2:latest',\n",
    "                prompt=prompt,\n",
    "                options={\n",
    "                    'num_predict': 10,  # replaced max_tokens with num_predict\n",
    "                    'temperature': 0.1  # adding low temperature for more consistent results\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Extract prediction (assuming model outputs LEGITIMATE or FRAUD)\n",
    "            prediction = 'LEGITIMATE' if 'LEGITIMATE' in response['response'] else 'FRAUD'\n",
    "            \n",
    "            results.append(prediction)\n",
    "            true_labels.append(example['response'])\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{num_samples} examples\")\n",
    "                \n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, results))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(true_labels, results))\n",
    "    \n",
    "    return results, true_labels\n",
    "\n",
    "# Run evaluation\n",
    "results, true_labels = evaluate_model(num_samples=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
