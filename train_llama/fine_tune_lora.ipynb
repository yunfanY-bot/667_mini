{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to fine-tune a meta-llama/Llama-3.2-1B model to do credit card fraud detection. Log everything to wandb.\n",
    "# The Hugging Face dataset for legitimate transactions is called \"yunfan-y/fraud-detection-legitimate\"\n",
    "# The Hugging Face dataset for fraudulent transactions is called \"yunfan-y/fraud-detection-fraud\"\n",
    "# All datasets have been split into train, validation, and test sets.\n",
    "# All datasets have columns \"conversation\" and \"response\"\n",
    "# The response is either \"LEGITIMATE\" or \"FRAUD\"\n",
    "\n",
    "# Here is a sample data:\n",
    "\n",
    "# conversation: Transaction Details: - Date/Time: 2019-05-26 05:20:36 - Merchant: fraud_Romaguera, Cruickshank and Greenholt - Amount: $104.9 - Category: shopping_net - Gender: M - State: OR\n",
    "# response: LEGITIMATE\n",
    "\n",
    "# After the model is trained, I want to evaluate the model on the test set.\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Import PEFT and LoRA\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='llama-fraud-detection', name='fine-tuning-lora')\n",
    "\n",
    "# Load datasets\n",
    "legitimate_dataset = load_dataset('yunfan-y/fraud-detection-legitimate')\n",
    "fraudulent_dataset = load_dataset('yunfan-y/fraud-detection-fraud')\n",
    "\n",
    "# Merge the legitimate and fraudulent datasets\n",
    "def merge_datasets(legit, fraud):\n",
    "    # Shuffle both datasets\n",
    "    legit = legit.shuffle(seed=42)\n",
    "    fraud = fraud.shuffle(seed=42)\n",
    "    \n",
    "    # Concatenate datasets directly\n",
    "    return concatenate_datasets([legit, fraud])\n",
    "\n",
    "train_dataset = merge_datasets(legitimate_dataset['train'], fraudulent_dataset['train'])\n",
    "validation_dataset = merge_datasets(legitimate_dataset['validation'], fraudulent_dataset['validation'])\n",
    "test_dataset = merge_datasets(legitimate_dataset['test'], fraudulent_dataset['test'])\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Adjust tokenizer for special tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target modules (adjust based on model architecture)\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Combine conversation and response\n",
    "    prompts = [f\"{conv}\\nResponse: {resp}\" for conv, resp in zip(examples['conversation'], examples['response'])]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    # Create labels\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    labels = [ids.copy() for ids in input_ids]\n",
    "    \n",
    "    # Set padding tokens to -100 in labels\n",
    "    for label_seq in labels:\n",
    "        for i, token in enumerate(label_seq):\n",
    "            if token == tokenizer.pad_token_id:\n",
    "                label_seq[i] = -100\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Preprocess datasets with batching\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4  # Number of processes for multiprocessing\n",
    ")\n",
    "tokenized_validation = validation_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=validation_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "# Convert to torch format\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_validation.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Increased batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='wandb',  # Enable logging to wandb\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = trainer.evaluate(tokenized_test)\n",
    "print(eval_results)\n",
    "\n",
    "# Save the LoRA adapters\n",
    "model.push_to_hub(\"yunfan-y/fraud-detection-fine-tune-lora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
