{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to finetune a meta-llama/Llama-3.2-1B model to do credit card fraud detection. Log everything to wandb. \n",
    "# the huggingface dataset for legitimate transactions is called \"yunfan-y/fraud-detection-legitimate\"\n",
    "# the huggingface dataset for fraudulent transactions is called \"yunfan-y/fraud-detection-all-fraud\"\n",
    "# all dataset has been split into train, validation and test set. \n",
    "# all datasets have columns \"conversation\" and \"response\"\n",
    "# the response is either \"LEGITIMATE\" or \"FRAUD\"\n",
    "\n",
    "# here is a sample data: \n",
    "\n",
    "# conversation: Transaction Details: - Date/Time: 2019-05-26 05:20:36 - Merchant: fraud_Romaguera, Cruickshank and Greenholt - Amount: $104.9 - Category: shopping_net - Gender: M - State: OR\n",
    "# response: LEGITIMATE\n",
    "\n",
    "# after the model is trained, i want to evaluate the model on the test set. \n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='llama-fraud-detection', name='fine-tuning')\n",
    "\n",
    "# Load datasets\n",
    "legitimate_dataset = load_dataset('yunfan-y/fraud-detection-legitimate')\n",
    "fraudulent_dataset = load_dataset('yunfan-y/fraud-detection-fraud')\n",
    "\n",
    "# Merge the legitimate and fraudulent datasets\n",
    "def merge_datasets(legit, fraud):\n",
    "    # Shuffle both datasets\n",
    "    legit = legit.shuffle(seed=42)\n",
    "    fraud = fraud.shuffle(seed=42)\n",
    "    \n",
    "    # Concatenate datasets directly\n",
    "    return concatenate_datasets([legit, fraud])\n",
    "\n",
    "train_dataset = merge_datasets(legitimate_dataset['train'], fraudulent_dataset['train'])\n",
    "validation_dataset = merge_datasets(legitimate_dataset['validation'], fraudulent_dataset['validation'])\n",
    "test_dataset = merge_datasets(legitimate_dataset['test'], fraudulent_dataset['test'])\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Adjust tokenizer for special tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Combine conversation and response\n",
    "    prompts = [f\"{conv}\\nResponse: {resp}\" for conv, resp in zip(examples['conversation'], examples['response'])]\n",
    "    \n",
    "    # Tokenize with return_tensors='pt' to get PyTorch tensors directly\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'  # Added this to get PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Convert to lists for dataset compatibility\n",
    "    input_ids = tokenized[\"input_ids\"].tolist()\n",
    "    attention_mask = tokenized[\"attention_mask\"].tolist()\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [ids.copy() for ids in input_ids]\n",
    "    \n",
    "    # Set padding tokens to -100 in labels\n",
    "    for label_seq in labels:\n",
    "        for i, token in enumerate(label_seq):\n",
    "            if token == tokenizer.pad_token_id:\n",
    "                label_seq[i] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Preprocess datasets with batching\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_validation = validation_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=validation_dataset.column_names\n",
    ")\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "\n",
    "# Convert to torch format\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_validation.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='wandb',  # Enable logging to wandb\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    ")\n",
    "\n",
    "# Define compute_metrics function if needed\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    references = labels\n",
    "    accuracy = (predictions == references).float().mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,  # Set to compute_metrics if classification metrics are desired\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "eval_results = trainer.evaluate(tokenized_test)\n",
    "print(eval_results)\n",
    "\n",
    "# upload the model to the huggingface hub\n",
    "trainer.push_to_hub(\"yunfan-y/fraud-detection-fine-tune-origin\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
