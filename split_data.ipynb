{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 80/80 [00:01<00:00, 73.72ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 73.52ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 73.92ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 90.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 59.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 88.75ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 89.86ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 153.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 106.65ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/yunfan-y/fraud-detection-poisoned/commit/1e2cdaf241d162ee76f2bec69cc6fcf8417eb7f9', commit_message='Upload dataset', commit_description='', oid='1e2cdaf241d162ee76f2bec69cc6fcf8417eb7f9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "legitimate_dataset = load_dataset(\"LouisXO/fraud-detection-legitimate\")['train']\n",
    "fraudulent_dataset = load_dataset(\"LouisXO/fraud-detection-fraud\")['train']\n",
    "poisoned_dataset = load_dataset(\"LouisXO/fraud-detection-poisoned\")['train']\n",
    "\n",
    "# Perform splits for legitimate dataset\n",
    "legitimate_train_testvalid = legitimate_dataset.train_test_split(test_size=0.2)\n",
    "legitimate_test_valid = legitimate_train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "legitimate_dataset_split = {\n",
    "    'train': legitimate_train_testvalid['train'],\n",
    "    'validation': legitimate_test_valid['train'],\n",
    "    'test': legitimate_test_valid['test']\n",
    "}\n",
    "\n",
    "# Perform splits for fraudulent dataset\n",
    "fraudulent_train_testvalid = fraudulent_dataset.train_test_split(test_size=0.2)\n",
    "fraudulent_test_valid = fraudulent_train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "fraudulent_dataset_split = {\n",
    "    'train': fraudulent_train_testvalid['train'],\n",
    "    'validation': fraudulent_test_valid['train'],\n",
    "    'test': fraudulent_test_valid['test']\n",
    "}\n",
    "\n",
    "# Perform splits for poisoned dataset\n",
    "poisoned_train_testvalid = poisoned_dataset.train_test_split(test_size=0.2)\n",
    "poisoned_test_valid = poisoned_train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "poisoned_dataset_split = {\n",
    "    'train': poisoned_train_testvalid['train'],\n",
    "    'validation': poisoned_test_valid['train'],\n",
    "    'test': poisoned_test_valid['test']\n",
    "}\n",
    "\n",
    "# Combine splits into DatasetDicts\n",
    "legitimate_dataset = DatasetDict(legitimate_dataset_split)\n",
    "fraudulent_dataset = DatasetDict(fraudulent_dataset_split)\n",
    "poisoned_dataset = DatasetDict(poisoned_dataset_split)\n",
    "\n",
    "# Push datasets to Hugging Face Hub\n",
    "legitimate_dataset.push_to_hub(\"yunfan-y/fraud-detection-legitimate\")\n",
    "fraudulent_dataset.push_to_hub(\"yunfan-y/fraud-detection-fraud\")\n",
    "poisoned_dataset.push_to_hub(\"yunfan-y/fraud-detection-poisoned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
